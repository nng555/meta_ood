{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN='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'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_excepthook(exc_type, exc_value, exc_traceback):\n",
    "    traceback.print_exception(exc_type, exc_value, exc_traceback, show_locals=False)\n",
    "\n",
    "sys.excepthook = custom_excepthook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16,\n",
    "                                                 token=HF_TOKEN).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print_mem()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(src_dataset, src_config, src_split, tgt_dataset, tgt_config, tgt_split, seed):\n",
    "    src_data = load_dataset(src_dataset, src_config, split=src_split, trust_remote_code=True).shuffle(seed=seed)\n",
    "    tgt_data = load_dataset(tgt_dataset, tgt_config, trust_remote_code=True).shuffle(seed=seed)\n",
    "    tgt_data = tgt_data.filter(lambda x: x['label'] not in ['2', 2, 'neutral'])\n",
    "\n",
    "    if len(tgt_data.keys()) == 1:\n",
    "        tgt_data = tgt_data[tgt_split].train_test_split(test_size=0.2)\n",
    "    \n",
    "    return src_data, tgt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(i, src_exemplars, tgt_exemplars, n_id, n_ood, src_data, tgt_data,\n",
    "                    src_prompt, tgt_prompt, src_field, tgt_field, src_label_map, tgt_split):\n",
    "    src_idxs = src_exemplars[i, :n_id]\n",
    "    tgt_idxs = tgt_exemplars[i, :n_ood]\n",
    "\n",
    "    src_exs = []\n",
    "    tgt_exs = []\n",
    "\n",
    "    for src_idx in src_idxs:\n",
    "        src_idx = int(src_idx)\n",
    "        data = src_data[src_idx]\n",
    "        label = src_label_map[data['label']]\n",
    "        src_exs.append(src_prompt.format(data[src_field], label))\n",
    "\n",
    "    for tgt_idx in tgt_idxs:\n",
    "        tgt_idx = int(tgt_idx)\n",
    "        tgt_exs.append(tgt_prompt.format(tgt_data[tgt_split][tgt_idx][tgt_field]))\n",
    "\n",
    "    full_sys_prompt = OOD_ICL_PROMPT +\"### Source Examples\\n\\n\" + ''.join(src_exs) + \"### Target Examples\\n\\n\" + ''.join(tgt_exs)\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': full_sys_prompt},\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, messages, test_prompt, tok_choices, n_tgt_test,\n",
    "                   bsize, tgt_data, tgt_test_split, tgt_field, tgt_label_map):\n",
    "    bid = 0\n",
    "    ncorrect = 0\n",
    "\n",
    "    while bid < n_tgt_test:\n",
    "        batch = []\n",
    "        labels = []\n",
    "\n",
    "        while len(batch) < bsize and bid < n_tgt_test:\n",
    "            ex = tgt_data[tgt_test_split][bid]\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages + [\n",
    "                    {'role': 'user', 'content': test_prompt.format(ex[tgt_field])},\n",
    "                    {'role': 'system', 'content': \"Sentiment:\\n\"}\n",
    "                ],\n",
    "                add_generation_prompt=False,\n",
    "                return_tensors='pt',\n",
    "            ).cuda()\n",
    "            batch.append({'input_ids': inputs[0][:-1]})\n",
    "            labels.append(tgt_label_map[ex['label']])\n",
    "            bid += 1\n",
    "\n",
    "        ex_lens = [len(bid['input_ids']) - 1 for bid in batch]\n",
    "        batch = DataCollatorWithPadding(tokenizer)(batch)\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "\n",
    "        preds = out.logits[torch.arange(out.logits.shape[0]), ex_lens][:, tok_choices].argmax(-1).cpu().numpy()\n",
    "        ncorrect += (preds == labels).sum()\n",
    "        del out\n",
    "\n",
    "    return ncorrect / n_tgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_name: str='meta-llama/Llama-3.2-1B-Instruct',\n",
    "src_dataset: str='stanfordnlp/sst2',\n",
    "src_config: str=None,\n",
    "src_split: str='train',\n",
    "src_field: str='sentence',\n",
    "tgt_dataset: str='takala/financial_phrasebank',\n",
    "tgt_config: str = 'sentences_50agree',\n",
    "tgt_split: str='train',\n",
    "tgt_test_split: str='test',\n",
    "tgt_field: str='sentence',\n",
    "max_src_icl: int=16,\n",
    "max_tgt_icl: int=16,\n",
    "n_tgt_test: int=600,\n",
    "nsamples: int=10,\n",
    "seed: int=1,\n",
    "bsize: int=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompts\n",
    "src_prompt = \"Text:\\n{}\\n\\nSentiment:\\n{}\\n\\n\"\n",
    "tgt_prompt = \"Text:\\n{}\\n\\n\"\n",
    "test_prompt = \"Text:\\n{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)\n",
    "tok_choices = tokenizer.convert_tokens_to_ids([\"negative\", \"positive\"])\n",
    "\n",
    "# Load data\n",
    "src_data, tgt_data = load_data(src_dataset, src_config, src_split, tgt_dataset, tgt_config, tgt_split, seed)\n",
    "src_label_map = get_label_map(src_data[0]['label'])\n",
    "tgt_label_map = get_label_map(tgt_data[tgt_split][0]['label'])\n",
    "\n",
    "# Sample exemplars\n",
    "np.random.seed(seed)\n",
    "src_exemplars = np.random.choice(len(src_data), size=(nsamples, max_src_icl), replace=False)\n",
    "tgt_exemplars = np.random.choice(len(tgt_data[tgt_split]), size=(nsamples, max_tgt_icl), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_id = int(np.ceil(np.log2(max_src_icl)))\n",
    "max_log_ood = int(np.ceil(np.log2(max_tgt_icl)))\n",
    "\n",
    "res = np.zeros((max_log_id + 2, max_log_ood + 2, nsamples))\n",
    "out_name = src_dataset.split('/')[-1] + '_' + tgt_dataset.split('/')[-1]\n",
    "\n",
    "for n_log_id in list(range(max_log_id + 2)): # need at least one ID ICL\n",
    "    for n_log_ood in range(max_log_ood + 2):\n",
    "        if n_log_id == 0:\n",
    "            n_id = 0\n",
    "        else:\n",
    "            n_id = 2**(n_log_id - 1)\n",
    "\n",
    "        if n_log_ood == 0:\n",
    "            n_ood = 0\n",
    "        else:\n",
    "            n_ood = 2**(n_log_ood - 1)\n",
    "\n",
    "        for i in tqdm(range(nsamples), desc=f\"n_id: {n_id}, n_ood: {n_ood}\"):\n",
    "            # generate prompts\n",
    "            messages = generate_prompt(i, src_exemplars, tgt_exemplars, n_id, n_ood, src_data, tgt_data,\n",
    "                                          src_prompt, tgt_prompt, src_field, tgt_field, src_label_map, tgt_split)\n",
    "\n",
    "            # evaluate model\n",
    "            res[n_log_id, n_log_ood, i] = evaluate_model(model, tokenizer, messages, test_prompt, tok_choices, n_tgt_test,\n",
    "                                                         bsize, tgt_data, tgt_test_split, tgt_field, tgt_label_map)\n",
    "\n",
    "        print(res.mean(-1))\n",
    "        print(res.var(-1))\n",
    "        np.save(out_name + '.npy', res, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
