{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(id_dataset, id_config, id_split, ood_dataset, ood_config, seed):\n",
    "    id_data = load_dataset(id_dataset, id_config, split=id_split, trust_remote_code=True).shuffle(seed=seed)\n",
    "    ood_data = load_dataset(ood_dataset, ood_config, trust_remote_code=True).shuffle(seed=seed)\n",
    "\n",
    "    if len(ood_data.keys()) == 1:\n",
    "        ood_data = ood_data[\"train\"].train_test_split(test_size=0.2)\n",
    "    \n",
    "    return id_data, ood_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(i, id_data, ood_data, id_exemplars, ood_exemplars, n_id, n_ood, id_prompt, ood_prompt):\n",
    "    id_idxs = id_exemplars[i, :n_id]\n",
    "    ood_idxs = ood_exemplars[i, :n_ood]\n",
    "\n",
    "    id_exs = []\n",
    "    ood_exs = []\n",
    "\n",
    "    for id_idx in id_idxs:\n",
    "        id_idx = int(id_idx)\n",
    "        data = id_data[id_idx]\n",
    "        label = \"Positive\" if data['label'] else \"Negative\"\n",
    "        id_exs.append(id_prompt.format(data['text'], label))\n",
    "\n",
    "    for ood_idx in ood_idxs:\n",
    "        ood_idx = int(ood_idx)\n",
    "        ood_exs.append(ood_prompt.format(ood_data[ood_split][ood_idx]['sentence']))\n",
    "\n",
    "    full_prompt = ''.join(id_exs) + ''.join(ood_exs)\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, ood_data, full_prompt, test_prompt, tok_choices, bsize, n_ood_test):\n",
    "    bid = 0\n",
    "    ncorrect = 0\n",
    "\n",
    "    while bid < n_ood_test:\n",
    "        batch = []\n",
    "        labels = []\n",
    "\n",
    "        while len(batch) < bsize and bid < n_ood_test:\n",
    "            batch.append(full_prompt + test_prompt.format(ood_data['test'][bid]['sentence']))\n",
    "            labels.append(ood_data['test'][bid]['label'])\n",
    "            bid += 1\n",
    "\n",
    "        batch = tokenizer(batch)\n",
    "        ex_lens = [len(bid) - 1 for bid in batch['input_ids']]\n",
    "        batch = DataCollatorWithPadding(tokenizer)(batch)\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "\n",
    "        preds = out.logits[torch.arange(out.logits.shape[0]), ex_lens][:, tok_choices].argmax(-1).cpu().numpy()\n",
    "        ncorrect += (preds == labels).sum()\n",
    "        del out\n",
    "\n",
    "    return ncorrect / n_ood_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "id_dataset = \"stanfordnlp/imdb\"\n",
    "id_config = None\n",
    "id_split = \"train\"\n",
    "ood_dataset = \"stanfordnlp/sst2\"\n",
    "ood_config = None\n",
    "ood_split = \"train\"\n",
    "max_id_icl = 16\n",
    "max_ood_icl = 16\n",
    "n_ood_test = 600\n",
    "nsamples = 10\n",
    "seed = 1\n",
    "bsize = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompts and label tokens\n",
    "id_prompt = \"Review: {}\\nSentiment: {}\\n\\n\"\n",
    "ood_prompt = \"Review: {}\\n\\nSentiment:\\n\\n\"\n",
    "test_prompt = \"Review: {}\\nSentiment:\"\n",
    "tok_choices = [51957, 45003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "id_data, ood_data = load_data(id_dataset, id_config, id_split, ood_dataset, ood_config, seed)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)\n",
    "\n",
    "# Sample exemplars\n",
    "np.random.seed(seed)\n",
    "id_exemplars = np.random.choice(len(id_data), size=(nsamples, max_id_icl), replace=False)\n",
    "ood_exemplars = np.random.choice(len(ood_data[ood_split]), size=(nsamples, max_ood_icl), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_id = int(np.ceil(np.log2(max_id_icl)))\n",
    "max_log_ood = int(np.ceil(np.log2(max_ood_icl)))\n",
    "\n",
    "res = np.zeros((max_log_id + 2, max_log_ood + 2, nsamples))\n",
    "out_name = id_dataset.split('/')[-1] + '_' + ood_dataset.split('/')[-1]\n",
    "\n",
    "for n_log_id in list(range(max_log_id + 2))[::-1]: # need at least one ID ICL\n",
    "    for n_log_ood in range(max_log_ood + 2):\n",
    "        if n_log_id == 0:\n",
    "            n_id = 0\n",
    "        else:\n",
    "            n_id = 2**(n_log_id - 1)\n",
    "\n",
    "        if n_log_ood == 0:\n",
    "            n_ood = 0\n",
    "        else:\n",
    "            n_ood = 2**(n_log_ood - 1)\n",
    "\n",
    "        for i in tqdm(range(nsamples), desc=f\"n_id: {n_id}, n_ood: {n_ood}\"):\n",
    "            # generate prompts\n",
    "            full_prompt = generate_prompts(i, id_data, ood_data, id_exemplars, ood_exemplars,\n",
    "                                           n_id, n_ood, id_prompt, ood_prompt)\n",
    "\n",
    "            # evaluate model\n",
    "            res[n_log_id, n_log_ood, i] = evaluate_model(model, tokenizer, ood_data, full_prompt, \n",
    "                                                         test_prompt, tok_choices, bsize, n_ood_test)\n",
    "\n",
    "        print(res.mean(-1))\n",
    "        print(res.var(-1))\n",
    "        np.save(out_name + '.npy', res, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl_ood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
