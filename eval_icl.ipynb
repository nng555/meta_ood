{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token='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'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(id_dataset, id_config, id_split, ood_dataset, ood_config, seed):\n",
    "    id_data = load_dataset(id_dataset, id_config, split=id_split, trust_remote_code=True).shuffle(seed=seed)\n",
    "    ood_data = load_dataset(ood_dataset, ood_config, trust_remote_code=True).shuffle(seed=seed)\n",
    "\n",
    "    if len(ood_data.keys()) == 1:\n",
    "        ood_data = ood_data[\"train\"].train_test_split(test_size=0.2)\n",
    "    \n",
    "    return id_data, ood_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, token=hf_token).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(i, id_data, ood_data, id_exemplars, ood_exemplars, n_id, n_ood, id_prompt, ood_prompt):\n",
    "    id_idxs = id_exemplars[i, :n_id]\n",
    "    ood_idxs = ood_exemplars[i, :n_ood]\n",
    "\n",
    "    id_exs = []\n",
    "    ood_exs = []\n",
    "\n",
    "    for id_idx in id_idxs:\n",
    "        id_idx = int(id_idx)\n",
    "        data = id_data[id_idx]\n",
    "        label = \"Positive\" if data['label'] else \"Negative\"\n",
    "        id_exs.append(id_prompt.format(data['text'], label))\n",
    "\n",
    "    for ood_idx in ood_idxs:\n",
    "        ood_idx = int(ood_idx)\n",
    "        ood_exs.append(ood_prompt.format(ood_data[ood_split][ood_idx]['sentence']))\n",
    "\n",
    "    full_prompt = ''.join(id_exs) + ''.join(ood_exs)\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, ood_data, full_prompt, test_prompt, tok_choices, bsize, n_ood_test):\n",
    "    bid = 0\n",
    "    ncorrect = 0\n",
    "\n",
    "    while bid < n_ood_test:\n",
    "        batch = []\n",
    "        labels = []\n",
    "\n",
    "        while len(batch) < bsize and bid < n_ood_test:\n",
    "            batch.append(full_prompt + test_prompt.format(ood_data['test'][bid]['sentence']))\n",
    "            labels.append(ood_data['test'][bid]['label'])\n",
    "            bid += 1\n",
    "\n",
    "        batch = tokenizer(batch, truncation=True, padding='max_length', max_length=1024, return_tensors='pt')\n",
    "        ex_lens = [len(bid) - 1 for bid in batch['input_ids']]\n",
    "        batch = DataCollatorWithPadding(tokenizer)(batch)\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "\n",
    "        preds = out.logits[torch.arange(out.logits.shape[0]), ex_lens][:, tok_choices].argmax(-1).cpu().numpy()\n",
    "        ncorrect += (preds == labels).sum()\n",
    "        del out\n",
    "\n",
    "    return ncorrect / n_ood_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "id_dataset = \"stanfordnlp/imdb\"\n",
    "id_config = None\n",
    "id_split = \"train\"\n",
    "ood_dataset = \"stanfordnlp/sst2\"\n",
    "ood_config = None\n",
    "ood_split = \"train\"\n",
    "max_id_icl = 16\n",
    "max_ood_icl = 16\n",
    "n_ood_test = 600\n",
    "nsamples = 10\n",
    "seed = 1\n",
    "bsize = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompts and label tokens\n",
    "id_prompt = \"Review: {}\\nSentiment: {}\\n\\n\"\n",
    "ood_prompt = \"Review: {}\\n\\nSentiment:\\n\\n\"\n",
    "test_prompt = \"Review: {}\\nSentiment:\"\n",
    "tok_choices = [51957, 45003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23094e6b456747e3be2c45a198026ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "id_data, ood_data = load_data(id_dataset, id_config, id_split, ood_dataset, ood_config, seed)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Sample exemplars\n",
    "np.random.seed(seed)\n",
    "id_exemplars = np.random.choice(len(id_data), size=(nsamples, max_id_icl), replace=False)\n",
    "ood_exemplars = np.random.choice(len(ood_data[ood_split]), size=(nsamples, max_ood_icl), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_id: 16, n_ood: 0:   0%|          | 0/10 [00:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m generate_prompts(i, id_data, ood_data, id_exemplars, ood_exemplars,\n\u001b[1;32m     22\u001b[0m                                    n_id, n_ood, id_prompt, ood_prompt)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# evaluate model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     res[n_log_id, n_log_ood, i] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mood_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_choices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ood_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, ood_data, full_prompt, test_prompt, tok_choices, bsize, n_ood_test)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 20\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_lens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_choices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     21\u001b[0m ncorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_log_id = int(np.ceil(np.log2(max_id_icl)))\n",
    "max_log_ood = int(np.ceil(np.log2(max_ood_icl)))\n",
    "\n",
    "res = np.zeros((max_log_id + 2, max_log_ood + 2, nsamples))\n",
    "out_name = id_dataset.split('/')[-1] + '_' + ood_dataset.split('/')[-1]\n",
    "\n",
    "for n_log_id in list(range(max_log_id + 2))[::-1]: # need at least one ID ICL\n",
    "    for n_log_ood in range(max_log_ood + 2):\n",
    "        if n_log_id == 0:\n",
    "            n_id = 0\n",
    "        else:\n",
    "            n_id = 2**(n_log_id - 1)\n",
    "\n",
    "        if n_log_ood == 0:\n",
    "            n_ood = 0\n",
    "        else:\n",
    "            n_ood = 2**(n_log_ood - 1)\n",
    "\n",
    "        for i in tqdm(range(nsamples), desc=f\"n_id: {n_id}, n_ood: {n_ood}\"):\n",
    "            # generate prompts\n",
    "            full_prompt = generate_prompts(i, id_data, ood_data, id_exemplars, ood_exemplars,\n",
    "                                           n_id, n_ood, id_prompt, ood_prompt)\n",
    "\n",
    "            # evaluate model\n",
    "            res[n_log_id, n_log_ood, i] = evaluate_model(model, tokenizer, ood_data, full_prompt, \n",
    "                                                         test_prompt, tok_choices, bsize, n_ood_test)\n",
    "\n",
    "        print(res.mean(-1))\n",
    "        print(res.var(-1))\n",
    "        np.save(out_name + '.npy', res, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
